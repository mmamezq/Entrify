1. Tokenize inputs using BERT (input_ids are the processed data to be used as input to LSTM model)
2. Pad Sequences and Truncate based on desired sequence length
    Types of padding:
        1. Determine maximum sequence length:
            - can select 95th percentile of sequence lengths in dataframe.
        2. Fixed vs. Dynamic Padding:
            - Padding length can be fixed where all sequences are padded to the same maximum length (as above)
            - Dynamic padding: where each sequence is padded to its own length within a batch
    Our approach:
        - Because we develop and train our own LSTM from scratch, we use the 95th percentile
          of sequence length for the padding/truncation process.
        - This approach compromises between retaining as much data as possible while compensating for computing resource availabilty

        - NOTE: This selection may be finetuned and changed upon model training.
3. Convert token ids to embeddings
    - We can either use embeddings from pre-trained models such as BERT or initialize
      embeddings randomly within the embedding layer of the LSTM (or other NLP) model
    Our approach:
        - We utilize BERT's token id's as input to a pre-trained BERT model (bert-uncased-base).
        - We use the output of BERT as input to the embedding layer of the LSTM model.